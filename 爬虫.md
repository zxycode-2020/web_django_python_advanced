爬虫
====

1. 爬虫的构成
    1. 下载器: fetcher
    2. 解析器: parser
    3. 处理器: processor
    3. 调度器: scheduler
        - 进程 / 线程 / 协程
        - Celery
        - Airflow
    4. 管理后台

2. 怎样开始一个爬虫
    1. 确定抓取目标
    2. 页面分析
        1. pyquery

                doc = pq(filename='./Python爬虫.htm')
                doc('a.position_link>h3')
                doc('li.con_list_item')('a')('h3')

        2. xpath

                e = etree.HTML(html)
                e.xpath('//li//a[@class="position_link"]//h3/text()')

        3. json
        4. regexp
        5. 练习
            1. 通过搜狗的公众号搜索，进入公众号文章列表页面，并保存成文件
            2. 通过 pyquery 过滤出：公众号名称、文章 ID、文章标题、文章链接
            3. 打开一篇微信公众号的文章，并保存成文件
            4. 通过 xpath 过滤出：文章名称、文章内容、作者、日期
    3. 流程规划
    4. 存储规划
        - 文件
        - 数据库
        - 其他

3. 文明抓取: robots.txt
    - 告诉搜索引擎(网络蜘蛛)网站中的哪些内容可以被抓取, 哪些是不应被抓取的
    - robots.txt 不是一个规范，而只是约定俗成的，有些搜索引擎会遵守这一规范，有些则不然
    - [robots.txt 的写法](https://support.google.com/webmasters/answer/6062596?hl=zh-Hans)

            User-agent: *
            Disallow: /xxx
            Allow: /yyy

4. 常遇到的问题
    1. JS 页面渲染
        - selenium + Phantomjs
        - scrapy-splash + splash
    2. 反爬虫策略
        1. UserAgent 限制
            - 记录常见的浏览器的 UserAgent
        2. Cookie 限制
            - 清空当前 cookie
            - 对于需要登陆的网站, 清除 cookie 后需要重新登录, 此时可能需要大量账号
        3. IP 限制
            - 抓取代理 IP 库
            - 代理分为匿名代理和透明代理
            - socks5 代理协议

                    proxy = {'http': 'socks5://127.0.0.1:1080',
                             'https': 'socks5://127.0.0.1:1080'}
                    requests.get(url, proxies=proxy)

            - tor

        4. 验证码
            - 依靠前面几步降低触发 anti-scrape 的概率
            - 降低抓取频率
            - 其他方式: 图像识别

    3. 练习: 通过 selenium + Phantomjs 抓取微信公众号文章
        1. 解析文章列表页面，提取出以下元素: 公众号名称、公众号 ID、文章标题、文章链接
           并保存到如下结构中:

                    feed = {
                        'feed_name': 'xxx',
                        'feed_id': 'xxx',
                        'post_list': [
                            {
                                'title': 'xxxx',
                                'link': 'xxxxx'
                            },
                            {
                                'title': 'xxxx',
                                'link': 'xxxxx'
                            },
                            ...
                        ]
                    }

        2. 根据解析到的文章链接，进一步抓取文章。提取文章的以下元素: 文章标题、作者、创建时间、文章内容
           将文件保存到如下数据结构：

                    post = {
                        'title': 'xxxx',
                        'auth': 'xxxx',
                        'date': 'xxxx',
                        'content': 'xxxx'
                    }

            并将 post 更新到 feed['post_list'] 中对应的数据中去

5. 任务调度
    1. 多线程
    2. 多进程
    3. RPC
    4. Queue

6. 分布式爬虫
    - 负载均衡
    - 多机协作: 消息队列

7. 常用库介绍
    - pyquery
    - lxml
    - BeautifulSoup4
    - urllib
    - pycurl
    - requests
    - pysocks
    - peewee

8. 练习: 抓取手机搜狐全部链接
    1. 递归下载手机搜狐的所有链接
    2. 相同链接只访问一次
    3. 只访问域名是 m.sohu.com 链接
    4. 数据清洗: 将抓到的链接规范化处理
    5. 并发执行
    6. 将结果存入 redis, 需要保存的内容:
        - Link
        - Link Name
        - HTTP Status Code

9. 一些成熟的爬虫框架
    - scrapy
    - pyspider
